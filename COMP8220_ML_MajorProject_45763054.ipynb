{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "COMP8220_ML_MajorProject_45763054.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vikramrajan28/MachineLearning_TweetEmotionDetection/blob/main/COMP8220_ML_MajorProject_45763054.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGR0Wprs7Q67"
      },
      "source": [
        "# <center>COMP7220/8220 Machine Learning</center>\n",
        "## <center> Major Project</center>\n",
        "##### <center> Vikram Rajan : 45763054</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJrwZ_KP7Q68"
      },
      "source": [
        "### INTRODUCTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KM1WkY0V7Q69"
      },
      "source": [
        "In this project, I have used the dataset related to language domain to perform a emotion type classification task on tweets data provided. The data is explored and preprocessed using various preprocessing techniques like removing stop words, lemmatisation, pos-tagging, etc. This project deals with the statistical approaches covering both conventional machine learning and deep learning methodologies/models.\n",
        " The dataset used here is the data provided from the Kaggle competition and also some external data from <a href=\"https://competitions.codalab.org/competitions/17751#learn_the_details\">the Affect in Tweets task at SemEval in 2018</a>.  \n",
        " \n",
        " **Conventional Model**  \n",
        " SVC model performed much better than other models with an accuracy of 68.8% during the Kaggle submission. The objective of a  *SVC (Support Vector Classifier)* is to fit to the data you provide, returning a \"best fit\" hyperplane that divides, or categorizes the data. After getting the hyperplane, we can then feed some features to our classifier to identify the \"predicted\" class. Other conventional models used for evaluation are *Logistic Regression, Naives Bayes and SGD  Clasiifier*.\n",
        " - TF-IDF vectorizer is used for feature extraction. It transforms the words  into its feature vectors which is then provided as input to the model. Similiar to CountVectorizer but performs well in this case.\n",
        "  \n",
        "**Deep Learning Model**  \n",
        " The Keras neural network model supports two main types of model such as *Sequential and Functional models*. Here we are using Sequential model which is a linear stack of layers. The Deep dense Neural Network  with a *Keras embedding layer and GlobalMaxPooling1D(a way to downsample the incoming feature vectors)* layer delivers maximum accuracy of 72.2% in Kaggle submissions for public test data. Keras embedding layer takes the previously claculated integers and maps them to a dense vector of the word embedding.\n",
        " - Loss functions used to compile the sequential model is *Categorical_crossentropy*, because the number of classes are four(anger, sad, fear and sadness).\n",
        " - Metrics used is *Categorical Accuracy*.\n",
        " - CNN model was also implemented but resulted in low accuracy over validation data compared to above model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3fKDr4W7Q69"
      },
      "source": [
        "#### Importing Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "np8wZEuP7Q6-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "dd4fa545-3846-40ab-aa70-7966ed0a60c7"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "from collections import defaultdict\n",
        "from textblob import TextBlob, Word\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "nltk.download('wordnet')\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from nltk import PorterStemmer\n",
        "from nltk.corpus import wordnet as wn\n",
        "from sklearn import preprocessing\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.metrics import classification_report\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "pd.set_option('max_colwidth', 400)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-48Pi6mu7Q7D"
      },
      "source": [
        "#### Importing Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJbr04zi7Q7E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "101ce5a5-24cb-4954-aaae-efa12a8e7159"
      },
      "source": [
        "# Preprocessed Dataset provided in the Kaggle Competition\n",
        "train_tweets = np.load('text_train_tweets.npy') # Train data\n",
        "train_labels = np.load('text_train_labels.npy') # Train Labels\n",
        "val_tweets = np.load('text_val_tweets.npy') # Validation data \n",
        "val_labels = np.load('text_val_labels.npy') # Validation Labels\n",
        "public_data = np.load('text_test_public_tweets_rand.npy') # Test public data\n",
        "print(\"Train Labels Y : \",len(train_labels),\"\\nTrain Tweets X : \",len(train_tweets))\n",
        "\n",
        "# Loading text word pickle file\n",
        "infile = open(\"text_word_to_idx.pkl\",'rb')\n",
        "old_dict = pickle.load(infile)\n",
        "infile.close()\n",
        "new_dict = dict([(value, key) for key, value in old_dict.items()]) \n",
        "\n",
        "# Function to append three dataframes\n",
        "def emMerge(x,y,z):\n",
        "    f1 =  x.append(y,ignore_index=True,sort=True)\n",
        "    f2 =  f1.append(z,ignore_index=True,sort=True)\n",
        "    return f2\n",
        "\n",
        "# Label Classificatrion:\n",
        "\n",
        "# Loading/Importing External Datasets\n",
        "anger = pd.read_excel('anger_train.xls')\n",
        "joy = pd.read_excel('joy.xls')\n",
        "fear = pd.read_excel('fear.xls')\n",
        "sad = pd.read_excel('sadness.xls')\n",
        "\n",
        "angert = pd.read_excel('anger-test-gold.xls')\n",
        "joyt = pd.read_excel('joy-test-gold.xls')\n",
        "feart = pd.read_excel('fear-test-gold.xls')\n",
        "sadt = pd.read_excel('sadness-test-gold.xls')\n",
        "\n",
        "angerd = pd.read_excel('anger-dev.xls')\n",
        "joyd = pd.read_excel('joy-dev.xls')\n",
        "feard = pd.read_excel('fear-dev.xls')\n",
        "sadd = pd.read_excel('sadness-dev.xls')\n",
        "\n",
        "\n",
        "angerSet = emMerge(anger,angert,angerd)\n",
        "joySet = emMerge(joy,joyt,joyd)\n",
        "fearSet = emMerge(fear,feart,feard)\n",
        "sadSet = emMerge(sad,sadt,sadd)\n",
        "\n",
        "\n",
        "f1 =  angerSet.append(fearSet,ignore_index=True,sort=True)\n",
        "f2 =  f1.append(joySet,ignore_index=True,sort=True)\n",
        "extdata =  f2.append(sadSet,ignore_index=True,sort=True)\n",
        "extdata['Content']=extdata['Tweet']\n",
        "extdata = extdata.drop('ID',1)\n",
        "extdata = extdata.drop('Intensity Class',1)\n",
        "extdata = extdata.drop('Tweet',1)\n",
        "print(\"External data shape: \",extdata.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Labels Y :  7098 \n",
            "Train Tweets X :  7098\n",
            "External data shape:  (12634, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylfMQDPI7Q7H"
      },
      "source": [
        "**Label Encoding** for external datasets as the labels are anger, sadness, joy and fear.\n",
        "- anger = 0 , fear = 1, joy = 2, sadness =3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbMRDea47Q7I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "13ee7d07-edac-4bd2-e064-1e0ea4ea4789"
      },
      "source": [
        "lbl_enc = preprocessing.LabelEncoder()\n",
        "extdata_y = lbl_enc.fit_transform(extdata['Affect Dimension'].values)\n",
        "extdata_y = pd.DataFrame(extdata_y)\n",
        "extdata['Sentiment']=extdata_y[0]\n",
        "extdata = extdata.drop('Affect Dimension',1)\n",
        "extdata['Content']=extdata['Content'].astype(str)\n",
        "extdata.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Content</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@xandraaa5 @amayaallyn6 shut up hashtags are cool #offended</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>it makes me so fucking irate jesus. nobody is calling ppl who like hajime abusive stop with the strawmen lmao</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Lol Adam the Bull with his fake outrage...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@THATSSHAWTYLO passed away early this morning in a fast and furious styled car crash as he was leaving an ATL strip club. That's rough stuff</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@Kristiann1125 lol wow i was gonna say really?! haha have you seen chris or nah? you dont even snap me anymore dude!</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                        Content  Sentiment\n",
              "0                                                                                   @xandraaa5 @amayaallyn6 shut up hashtags are cool #offended          0\n",
              "1                                 it makes me so fucking irate jesus. nobody is calling ppl who like hajime abusive stop with the strawmen lmao          0\n",
              "2                                                                                                    Lol Adam the Bull with his fake outrage...          0\n",
              "3  @THATSSHAWTYLO passed away early this morning in a fast and furious styled car crash as he was leaving an ATL strip club. That's rough stuff          0\n",
              "4                          @Kristiann1125 lol wow i was gonna say really?! haha have you seen chris or nah? you dont even snap me anymore dude!          0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YE52H7fk7Q7L"
      },
      "source": [
        "### Pre Processing "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzeJOH_A7Q7M"
      },
      "source": [
        "Preprocessing function for external datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfXtgcmz7Q7O"
      },
      "source": [
        "def preproccessingfnExt(dataset):\n",
        "    train_tweets_data=dataset\n",
        "    # making all the tweets lower case\n",
        "    train_tweets_data['Content'] = train_tweets_data['Content'].apply(lambda x: x.lower())\n",
        "    # Tokenizing the tweets\n",
        "    train_tweets_data['Content'] = train_tweets_data['Content'].apply(lambda x: word_tokenize(x))\n",
        "    \n",
        "    # Initialisisng the tag map for pos_tagging\n",
        "    tag_map = defaultdict(lambda : wn.NOUN) \n",
        "    tag_map['N'] = wn.NOUN\n",
        "    tag_map['J'] = wn.ADJ\n",
        "    tag_map['V'] = wn.VERB \n",
        "    tag_map['R'] = wn.ADV\n",
        "\n",
        "    for index, entry in enumerate(train_tweets_data['Content']):\n",
        "        final_words = [] \n",
        "        word_lemmatized = WordNetLemmatizer() # Initialising Lemmatizer\n",
        "        for word, tag in pos_tag(entry):\n",
        "            if word not in stopwords.words('english') and word.isalpha():# Removal of stop words and special characters\n",
        "                word_final = word_lemmatized.lemmatize(word, tag_map[tag[0]])\n",
        "                final_words.append(word_final) \n",
        "        train_tweets_data.loc[index, 'Content'] = str(final_words)\n",
        "    \n",
        "    # Reforming the sentence/tweet\n",
        "    train_tweets_data['Content'] = train_tweets_data['Content'].str.replace(',',' ')\n",
        "    train_tweets_data['Content'] = train_tweets_data['Content'].str.replace('[^\\w\\s]','')\n",
        "    \n",
        "    # Stemming\n",
        "    tokenized_tweet = train_tweets_data['Content'].apply(lambda x: x.split())\n",
        "    ps = PorterStemmer()\n",
        "    tokenized_tweet = tokenized_tweet.apply(lambda x: [ps.stem(i) for i in x])\n",
        "    # Reforming the final tweets for training\n",
        "    for i in range(len(tokenized_tweet)):\n",
        "        tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\n",
        "    train_tweets_data['Content'] = tokenized_tweet\n",
        "    \n",
        "    \n",
        "    return train_tweets_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dTuBHav7Q7R"
      },
      "source": [
        "Pre processing function for given training dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrO4h19r7Q7R"
      },
      "source": [
        "def preproccessingfn(dataset):\n",
        "    content =[]\n",
        "    # Regeneration of the tweets from the text_word_to_idx pickle file.\n",
        "    for j in dataset:\n",
        "      tw = contenttext(j)\n",
        "      content.append(tw)\n",
        "    \n",
        "    # Convereting the data to a dataframe.\n",
        "    train_tweets_df = pd.DataFrame(content)\n",
        "    # removal of null and other unnecessary words/values from the tweets.\n",
        "    train_tweets_df = train_tweets_df.replace( \"<NULL>\",np.nan)\n",
        "    train_tweets_df = train_tweets_df.replace( \"<START>\",np.nan)\n",
        "    train_tweets_df = train_tweets_df.replace( \"<END>\",np.nan)\n",
        "    train_tweets_df = train_tweets_df.replace( \"<user>\",np.nan)\n",
        "\n",
        "    train_tweets_df[0] = train_tweets_df[train_tweets_df.columns[:]].apply( lambda x: ','.join(x.dropna().astype(str)),axis=1)\n",
        "    train_tweets_data = pd.DataFrame()\n",
        "    train_tweets_data['Content1']=train_tweets_df[0]\n",
        "    # making all the tweets lower case\n",
        "    train_tweets_data['Content1'] = train_tweets_data['Content1'].apply(lambda x: x.lower())\n",
        "    # Tokenizing the tweets\n",
        "    train_tweets_data['Content1'] = train_tweets_data['Content1'].apply(lambda x: word_tokenize(x))\n",
        "    \n",
        "    # Initialisisng the tag map for pos_tagging\n",
        "    tag_map = defaultdict(lambda : wn.NOUN) \n",
        "    tag_map['J'] = wn.ADJ\n",
        "    tag_map['V'] = wn.VERB \n",
        "    tag_map['R'] = wn.ADV\n",
        "\n",
        "    for index, entry in enumerate(train_tweets_data['Content1']):\n",
        "        final_words = [] \n",
        "        word_lemmatized = WordNetLemmatizer() # Initialising Lemmatizer\n",
        "        for word, tag in pos_tag(entry):\n",
        "            if word not in stopwords.words('english') and word.isalpha():# Removal of stop words and special characters\n",
        "                word_final = word_lemmatized.lemmatize(word, tag_map[tag[0]])\n",
        "                final_words.append(word_final) \n",
        "        train_tweets_data.loc[index, 'Content'] = str(final_words)\n",
        "    \n",
        "    # Reforming the sentence/tweet\n",
        "    train_tweets_data['Content'] = train_tweets_data['Content'].str.replace(',',' ')\n",
        "    train_tweets_data['Content'] = train_tweets_data['Content'].str.replace('[^\\w\\s]','')\n",
        "    train_tweets_data = train_tweets_data.drop('Content1', 1)\n",
        "    \n",
        "    \n",
        "    # Stemming\n",
        "    tokenized_tweet = train_tweets_data['Content'].apply(lambda x: x.split())\n",
        "    ps = PorterStemmer()\n",
        "    tokenized_tweet = tokenized_tweet.apply(lambda x: [ps.stem(i) for i in x])\n",
        "    # Reforming the final tweets for training\n",
        "    for i in range(len(tokenized_tweet)):\n",
        "        tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\n",
        "    train_tweets_data['Content'] = tokenized_tweet\n",
        "    \n",
        "    return train_tweets_data\n",
        "\n",
        "def contenttext(x):\n",
        "  xyz=[]\n",
        "  for i in x:\n",
        "    xyz.append(new_dict[i])\n",
        "  return xyz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMaAJ7aN7Q7V"
      },
      "source": [
        "Processing external data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9vn5CbR7Q7W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "634f9dc1-2da8-4139-ee0d-4cba74b6c672"
      },
      "source": [
        "external_data = preproccessingfnExt(extdata)\n",
        "print(\"Shape of External data: \",external_data.shape)\n",
        "external_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of External data:  (12634, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Content</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>shut hashtag cool offend</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>make fuck irat jesu nobodi call ppl like hajim abus stop strawman lmao</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>lol adam bull fake outrag</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>thatsshawtylo pass away earli morn fast furiou style car crash leav atl strip club rough stuff</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>lol wow gon na say realli haha see chri nah dont even snap anymor dude</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                          Content  Sentiment\n",
              "0                                                                        shut hashtag cool offend          0\n",
              "1                          make fuck irat jesu nobodi call ppl like hajim abus stop strawman lmao          0\n",
              "2                                                                       lol adam bull fake outrag          0\n",
              "3  thatsshawtylo pass away earli morn fast furiou style car crash leav atl strip club rough stuff          0\n",
              "4                          lol wow gon na say realli haha see chri nah dont even snap anymor dude          0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPefkRa37Q7a"
      },
      "source": [
        "Processing the given train data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-gb-QYD7Q7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "42cc49cd-61ee-4195-ce6a-9ddcdf28349a"
      },
      "source": [
        "train_labels_Y = pd.DataFrame(train_labels, columns=['Sentiment'])\n",
        "train_data = preproccessingfn(train_tweets)\n",
        "\n",
        "# Joining the processed train data  and its labels into a dataframe.\n",
        "train_set = pd.concat([train_data, train_labels_Y], axis=1)\n",
        "print(\"Shape of given Train data: \",train_set.shape)\n",
        "train_set.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of given Train data:  (7098, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Content</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>make fuck irat jesu nobodi call ppl like hajim abus stop strawman lmao</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>lol adam bull fake outrag</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>pass away earli morn fast furiou style car crash leav atl strip club rough stuff</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>lol wow gon na say realli haha see chri nah dont even snap anymor dude</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>need sushi date oliv guard date rocki date</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                            Content  Sentiment\n",
              "0            make fuck irat jesu nobodi call ppl like hajim abus stop strawman lmao          0\n",
              "1                                                         lol adam bull fake outrag          0\n",
              "2  pass away earli morn fast furiou style car crash leav atl strip club rough stuff          0\n",
              "3            lol wow gon na say realli haha see chri nah dont even snap anymor dude          0\n",
              "4                                        need sushi date oliv guard date rocki date          0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QE64IQ3r7Q7e"
      },
      "source": [
        "Processing the given validation data(same as train data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCQuBQYI7Q7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "3343b25c-2b47-4370-8ce9-d09ce6e948ab"
      },
      "source": [
        "val_lab = pd.DataFrame(val_labels, columns=['Sentiment'])\n",
        "val_data = preproccessingfn(val_tweets)\n",
        "\n",
        "# Joining the processed validation data  and its labels into a dataframe.\n",
        "val_set = pd.concat([val_data, val_lab], axis=1)\n",
        "print(\"Shape of given Validation data: \",val_set.shape)\n",
        "val_set.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of given Validation data:  (1460, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Content</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>fume hijack money move full back</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>nightmar dream freedom</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>cnn realli need get busi number second fall soldier tragedi right back hatr potu</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>kikm horni kik nude girl horni snap</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>fuck tag pictur famili first cut number year ago one</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                            Content  Sentiment\n",
              "0                                                  fume hijack money move full back          0\n",
              "1                                                            nightmar dream freedom          0\n",
              "2  cnn realli need get busi number second fall soldier tragedi right back hatr potu          0\n",
              "3                                               kikm horni kik nude girl horni snap          0\n",
              "4                              fuck tag pictur famili first cut number year ago one          0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYBF37m37Q7i"
      },
      "source": [
        "Joining the provided train dataframe and the external dataframe to form a final large training dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2HawFVj7Q7i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "000349fe-a8e9-4b07-8117-865f2b6772c5"
      },
      "source": [
        "exttrain_set=train_set.append(external_data,ignore_index=True,sort=True)\n",
        "print(\"Shape of  Final Training data: \",exttrain_set.shape)\n",
        "exttrain_set.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of  Final Training data:  (19732, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Content</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>make fuck irat jesu nobodi call ppl like hajim abus stop strawman lmao</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>lol adam bull fake outrag</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>pass away earli morn fast furiou style car crash leav atl strip club rough stuff</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>lol wow gon na say realli haha see chri nah dont even snap anymor dude</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>need sushi date oliv guard date rocki date</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                            Content  Sentiment\n",
              "0            make fuck irat jesu nobodi call ppl like hajim abus stop strawman lmao          0\n",
              "1                                                         lol adam bull fake outrag          0\n",
              "2  pass away earli morn fast furiou style car crash leav atl strip club rough stuff          0\n",
              "3            lol wow gon na say realli haha see chri nah dont even snap anymor dude          0\n",
              "4                                        need sushi date oliv guard date rocki date          0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xK_bkmHx7Q7l"
      },
      "source": [
        "### Conventional Machine Learning Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMMp-27B7Q7m"
      },
      "source": [
        "The SVC model produced the best accuracy of __ during the Kaggle submission.\n",
        "\n",
        "Feature extraction is performed with the help of tf-idf feature extraction technique to convert the input tweets to its vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZAHNbFW7Q7m"
      },
      "source": [
        "# TF IDF Feature Extraction Function\n",
        "def featureExtract(para, host):\n",
        "    tf_idf = TfidfVectorizer(max_features=20000)\n",
        "    tf_idf.fit(host)\n",
        "    para_transformed = tf_idf.transform(para)\n",
        "    return para_transformed\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egXhb9sc7Q7p"
      },
      "source": [
        "Different Conventional training models were considered for training and the best one is selected with maximum validation accuracy.\n",
        "ML models are: Naives Bayes, SVC, SGDClassifier and Logistic regression model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uG8_ull57Q7p"
      },
      "source": [
        "**Naives Bayes Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWc6hhIQ7Q7q"
      },
      "source": [
        "def naivesBayesModel(comb,X):    \n",
        "    t_tf = featureExtract(X['Content'],comb['Content']) # Feature extraction for the train tweets\n",
        "    nb = MultinomialNB()\n",
        "    # Fit the training dataset.\n",
        "    nb.fit(t_tf, X['Sentiment'])\n",
        "    return nb\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7f_ug6g7Q7t"
      },
      "source": [
        "**SVC Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7y56G8iI7Q7t"
      },
      "source": [
        "def SVM_Model(comb,X):    \n",
        "    t_tf = featureExtract(X['Content'],comb['Content']) # Feature extraction for the train tweets\n",
        "    nb = SVC(C=10.0, kernel='linear', degree=3, gamma='scale')\n",
        "    # Fit the training dataset.\n",
        "    nb.fit(t_tf, X['Sentiment'])\n",
        "    return nb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vmvqK227Q7w"
      },
      "source": [
        "**SGD Classifier Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gku9yYC07Q7x"
      },
      "source": [
        "def SGDClassifierModel(comb,X):    \n",
        "    t_tf = featureExtract(X['Content'],comb['Content']) # Feature extraction for the train tweets\n",
        "    nb = SGDClassifier()\n",
        "    # Fit the training dataset.\n",
        "    nb.fit(t_tf, X['Sentiment'])\n",
        "    return nb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sjY9_X87Q71"
      },
      "source": [
        "**Logistic Regression Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67C7H5BL7Q72"
      },
      "source": [
        "def LogisticRegressionModel(comb,X):    \n",
        "    t_tf = featureExtract(X['Content'],comb['Content']) # Feature extraction for the train tweets\n",
        "    nb = LogisticRegression(max_iter=400)\n",
        "    # Fit the training dataset.\n",
        "    nb.fit(t_tf, X['Sentiment'])\n",
        "    return nb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qt6wTBjB7Q76"
      },
      "source": [
        "Function for fitting the model, evaluating accuracy over validation dataset and predicting the labels for test or private data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9_6wfoY7Q76"
      },
      "source": [
        "def predictionSets( X,dataset,ml,acScore ):\n",
        "    comb = X.append(dataset,ignore_index=True,sort=True)\n",
        "    model=None\n",
        "    # To identify which model is invoked\n",
        "    if ml=='NaivesBayes':\n",
        "        model=naivesBayesModel(comb,X)\n",
        "    elif ml=='SVM':\n",
        "        model=SVM_Model(comb,X)\n",
        "    elif ml=='SGD':\n",
        "        model=SGDClassifierModel(comb,X)\n",
        "    elif ml=='LogR':\n",
        "        model=LogisticRegressionModel(comb,X)\n",
        "    print(\"Processing Model: \",ml)\n",
        "    # Predict the labels on the validation dataset.\n",
        "    test = featureExtract(dataset['Content'],comb['Content'])\n",
        "    y_predict = model.predict(test)\n",
        "    # Use the accuracy_score function to get the accuracy,\n",
        "    if(acScore):\n",
        "        print(ml,\" Accuracy Score for \",ml,\" --> \", accuracy_score( dataset['Sentiment'],y_predict)*100)\n",
        "        target_names = ['anger', 'fear', 'joy', 'sadness']\n",
        "        print(ml,\" Classification_report \",ml,\"-->\\n\",classification_report(dataset['Sentiment'], y_predict, target_names=target_names))\n",
        "    return y_predict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtZQcdoj7Q79"
      },
      "source": [
        "Evaluating Models with respect to validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZseuY9v7Q7-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "15b42edb-9619-4f2e-db97-79163d370595"
      },
      "source": [
        "# Evaluating Naives Bayes Model:\n",
        "nbResult = predictionSets(exttrain_set,val_set,'NaivesBayes',True)\n",
        "\n",
        "# Evaluating SVC Model:\n",
        "svcResult = predictionSets(exttrain_set,val_set,'SVM',True)\n",
        "\n",
        "# Evaluating SGD CLASSIFIER Model:\n",
        "sgdResult = predictionSets(exttrain_set,val_set,'SGD',True)\n",
        "\n",
        "# Evaluating Logistic regression Model:\n",
        "logRResult = predictionSets(exttrain_set,val_set,'LogR',True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing Model:  NaivesBayes\n",
            "NaivesBayes  Accuracy Score for  NaivesBayes  -->  47.26027397260274\n",
            "NaivesBayes  Classification_report  NaivesBayes -->\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.48      0.42      0.45       387\n",
            "        fear       0.37      0.64      0.47       388\n",
            "         joy       0.76      0.56      0.65       289\n",
            "     sadness       0.48      0.30      0.37       396\n",
            "\n",
            "    accuracy                           0.47      1460\n",
            "   macro avg       0.53      0.48      0.48      1460\n",
            "weighted avg       0.51      0.47      0.47      1460\n",
            "\n",
            "Processing Model:  SVM\n",
            "SVM  Accuracy Score for  SVM  -->  54.31506849315069\n",
            "SVM  Classification_report  SVM -->\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.51      0.48      0.49       387\n",
            "        fear       0.45      0.57      0.50       388\n",
            "         joy       0.81      0.78      0.79       289\n",
            "     sadness       0.50      0.41      0.45       396\n",
            "\n",
            "    accuracy                           0.54      1460\n",
            "   macro avg       0.57      0.56      0.56      1460\n",
            "weighted avg       0.55      0.54      0.54      1460\n",
            "\n",
            "Processing Model:  SGD\n",
            "SGD  Accuracy Score for  SGD  -->  51.301369863013704\n",
            "SGD  Classification_report  SGD -->\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.51      0.44      0.47       387\n",
            "        fear       0.42      0.56      0.48       388\n",
            "         joy       0.72      0.71      0.72       289\n",
            "     sadness       0.47      0.40      0.43       396\n",
            "\n",
            "    accuracy                           0.51      1460\n",
            "   macro avg       0.53      0.53      0.53      1460\n",
            "weighted avg       0.52      0.51      0.51      1460\n",
            "\n",
            "Processing Model:  LogR\n",
            "LogR  Accuracy Score for  LogR  -->  51.50684931506849\n",
            "LogR  Classification_report  LogR -->\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.51      0.44      0.47       387\n",
            "        fear       0.42      0.60      0.49       388\n",
            "         joy       0.81      0.67      0.73       289\n",
            "     sadness       0.48      0.40      0.44       396\n",
            "\n",
            "    accuracy                           0.52      1460\n",
            "   macro avg       0.55      0.53      0.53      1460\n",
            "weighted avg       0.53      0.52      0.52      1460\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjI3NT367Q8A"
      },
      "source": [
        "Determining the best model: Comparing the accuracy of the above four models, its evident that SVC model performs much better with an accuracy of ~54%. Thus, the SVC model is considered as the best model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8SPfv-L7Q8B"
      },
      "source": [
        "**Best Parameters estimation** : Grid SearchCV is used for best parameter estimation for the best performing model, which is SVM model with max accuracy on validation data selected above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibvgYqfL7Q8B"
      },
      "source": [
        "# Estimation of best parameters for Logistic regression model\n",
        "def gridSearchCV(X,dataset):   \n",
        "    comb = X.append(dataset,ignore_index=True,sort=True)\n",
        "    t_tf = featureExtract(X['Content'],comb['Content'])\n",
        "    lr_params = {'C': [0.1,1, 10], 'gamma': [1,0.1,0.01,0.001],'kernel': ['rbf', 'poly', 'linear']}\n",
        "    # Define the gridsearchCV\n",
        "    lr_grid = GridSearchCV(SVC(), param_grid=lr_params, cv=3, refit=True,verbose=1)\n",
        "    lr_grid.fit(t_tf, X['Sentiment'])\n",
        "    print ('Best Score:', lr_grid.best_score_)\n",
        "    print (lr_grid.best_estimator_)\n",
        "    print ('Best Params:', lr_grid.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKWXzuAG7Q8D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "fbd876bd-4b0d-49b9-e0ed-a50c89648dc3"
      },
      "source": [
        "gridSearchCV(exttrain_set,val_set)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 108 out of 108 | elapsed: 55.2min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best Score: 0.8385805117134982\n",
            "SVC(C=10, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
            "    decision_function_shape='ovr', degree=3, gamma=1, kernel='rbf', max_iter=-1,\n",
            "    probability=False, random_state=None, shrinking=True, tol=0.001,\n",
            "    verbose=False)\n",
            "Best Params: {'C': 10, 'gamma': 1, 'kernel': 'rbf'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOwGv8n8MA6w"
      },
      "source": [
        "**Final model with improved parameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcBpbvsD7Q8J"
      },
      "source": [
        "# Overwritting the SVM model function\n",
        "def SVM_Model(comb,X):    \n",
        "    t_tf = featureExtract(X['Content'],comb['Content']) # Feature extraction for the train tweets\n",
        "    nb = SVC(C=10.0, kernel='rbf', degree=3, gamma='scale')\n",
        "    # Fit the training dataset.\n",
        "    nb.fit(t_tf, X['Sentiment'])\n",
        "    return nb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLw5xznM7Q8L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "b8f8c8cc-33c1-4344-aec8-5988d8eb0237"
      },
      "source": [
        "# Evaluating Naives Bayes Model:\n",
        "logRResult = predictionSets(exttrain_set,val_set,'SVM',True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing Model:  SVM\n",
            "SVM  Accuracy Score for  SVM  -->  56.91780821917808\n",
            "SVM  Classification_report  SVM -->\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.54      0.48      0.51       387\n",
            "        fear       0.46      0.58      0.51       388\n",
            "         joy       0.88      0.83      0.86       289\n",
            "     sadness       0.51      0.45      0.48       396\n",
            "\n",
            "    accuracy                           0.57      1460\n",
            "   macro avg       0.60      0.59      0.59      1460\n",
            "weighted avg       0.58      0.57      0.57      1460\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7YXgqt4MG62"
      },
      "source": [
        "After the use of best parameters, we can notice that the accuracy has increased from 54.32% to 56.91%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQrXemhP7Q8O"
      },
      "source": [
        "**Predicting the labels for public test dataset.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLM_4jtg7Q8O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "d848df14-5396-4e7a-dc6c-ceed18b328c9"
      },
      "source": [
        "# Preprocessing the test data similarly as the train data was performed\n",
        "pub_test_data = preproccessingfn(public_data)\n",
        "print(\"Shape of  Final public data: \",pub_test_data.shape)\n",
        "pub_test_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of  Final public data:  (4064, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>omg mother daughter dull ni move dad worri</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>happi birthday repeat miss excit back florida hear turkey date repeat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ever cri middl bomb rest someon wake emerg sleep cri</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>mental suffer worthless pain</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>courag driver shot bu show courag natur scare death threat make flee fast</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                     Content\n",
              "0                                 omg mother daughter dull ni move dad worri\n",
              "1      happi birthday repeat miss excit back florida hear turkey date repeat\n",
              "2                       ever cri middl bomb rest someon wake emerg sleep cri\n",
              "3                                               mental suffer worthless pain\n",
              "4  courag driver shot bu show courag natur scare death threat make flee fast"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51NriT097Q8R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d8332998-1f89-4efd-c96c-7b0c00aef9fb"
      },
      "source": [
        "# Function call to predict the labels for public dataset\n",
        "finTestPredit = predictionSets(exttrain_set,pub_test_data,'SVM',False)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing Model:  SVM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTtajj8S7Q8T"
      },
      "source": [
        "# Function to write the prediction to csv file inorder to upload the output into the Kaggle competition.\n",
        "def writeToCSV(result):\n",
        "    output = pd.DataFrame(result,columns=['Prediction'])\n",
        "    output.index += 1 \n",
        "    output.index.name='ID'\n",
        "    output.to_csv('public_45763054-conv.csv')\n",
        "    print(output.head())\n",
        "def writeToCSVpriv(result):\n",
        "    output = pd.DataFrame(result,columns=['Prediction'])\n",
        "    #output.index += 1 \n",
        "    output.index.name='ID'\n",
        "    output.to_csv('private_45763054-conv.csv')\n",
        "    print(output.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iFOv9DS7Q8V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "6a5039e5-8d03-4a7e-dfa0-79536a887cce"
      },
      "source": [
        "writeToCSV(finTestPredit)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    Prediction\n",
            "ID            \n",
            "1            3\n",
            "2            2\n",
            "3            3\n",
            "4            3\n",
            "5            1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqnXB-9M8EcW"
      },
      "source": [
        "**Predicting Private Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHKgK4SW8ROE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "199e2d5f-3c28-40d7-c4a6-db4355cd253d"
      },
      "source": [
        "private_data = np.load('text_test_private_tweets.npy')\n",
        "priv_data = preproccessingfn(private_data)\n",
        "print(\"Shape of  Final Private data: \",priv_data.shape)\n",
        "priv_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of  Final Private data:  (4257, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>whatev decid make sure make happi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>accept challeng liter even feel exhilar victori georg patton</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>roommat okay spell autocorrect terribl firstworldprob</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>cute atsu probabl shi photo cherri help uwu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>rooney fuck untouch fuck dread depay look decentishtonight</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                        Content\n",
              "0                             whatev decid make sure make happi\n",
              "1  accept challeng liter even feel exhilar victori georg patton\n",
              "2         roommat okay spell autocorrect terribl firstworldprob\n",
              "3                   cute atsu probabl shi photo cherri help uwu\n",
              "4    rooney fuck untouch fuck dread depay look decentishtonight"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPfmHxNZ8Muq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "191f8d0c-ddef-49fa-d3d0-8afb7ffcf843"
      },
      "source": [
        "finPrivatePredit = predictionSets(exttrain_set,priv_data,'SVM',False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing Model:  SVM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXZ4jm9U97G_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "3ace8885-9843-4865-d540-22b9675bfafc"
      },
      "source": [
        "writeToCSVpriv(finPrivatePredit)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    Prediction\n",
            "ID            \n",
            "0            2\n",
            "1            2\n",
            "2            1\n",
            "3            1\n",
            "4            1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_YIcbyL7Q8Y"
      },
      "source": [
        "### Deep Learning Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_n32bU277Q8Y"
      },
      "source": [
        "The final model that produced the best-performing predictions for the Kaggle submission (accuracy (x+5)%) was a fully connected dense model with __ NN layers. The input data is same preprocessed data used for conventional model comprising of both external and given train data together.\n",
        "For further processing unlike the above conventional model, here we have used keras tokeniser to convert the word in the tweets to index values according to the tokeniser. We have also converted the input labels into array/matrix as per the number of classes(anger,joy,fear and sadness) using *to_categorical* function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKFAeKb97Q8Z"
      },
      "source": [
        "# Copying the tweet contents and sentiment(labels) into  arrays.\n",
        "contents = exttrain_set[\"Content\"].values\n",
        "labels = exttrain_set[\"Sentiment\"].values\n",
        "labels = to_categorical(labels) # Conversion of training labels for input to the NN model\n",
        "\n",
        "# Train Validation split of the final dataset\n",
        "xtrain, xVal, y_train, y_val = train_test_split(contents, labels, test_size=0.20, random_state=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ds240CxH7Q8c"
      },
      "source": [
        "- **Here, for initial accuracy calculation to find the best model, I have used validation data as the test data and splitted final train data  for train-validation split.**  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnExM0h77Q8c"
      },
      "source": [
        "# Performing the same tasks in above step for validation data\n",
        "y_test = val_set[\"Sentiment\"].values\n",
        "Xtest = val_set[\"Content\"].values\n",
        "y_test = to_categorical(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiQtcvx17Q8f"
      },
      "source": [
        "# Use of Tokenizer for feature  extraction\n",
        "tokeniz = Tokenizer(num_words=5000)\n",
        "tokeniz.fit_on_texts(xtrain)\n",
        "\n",
        "X_train = tokeniz.texts_to_sequences(xtrain)\n",
        "X_test = tokeniz.texts_to_sequences(Xtest)\n",
        "X_val = tokeniz.texts_to_sequences(xVal)\n",
        "\n",
        "vocab_size = len(tokeniz.word_index) + 1  # Adding 1 because of reserved 0 index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IEWqoJl7Q8h"
      },
      "source": [
        "The transformed data using tokenizer in above step has rows with different lenths. Therefore applying padding of maxlenth of 50 to the matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cl-H0j4d7Q8h"
      },
      "source": [
        "maxlen = 50\n",
        "\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "X_val = pad_sequences(X_val, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxvaVzVs7Q8k"
      },
      "source": [
        "#### Creating the NN Model\n",
        "For Deep learning implementation, I have considered evaluating the deep dense NN network and CNN network. The below simple deeply dense neural network with 4 NN layers comprising an embedding layer, GlobalMaxpool layer, dense activation layer and finally a output softmax layer.\n",
        "This model is compiled with *Categorical crossentropy* loss function because there are four number of classes at the output dense layer.(anger, sadness, joy and fear). Softmax output layer is used as it can perform much better for more than two classifications. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5OmYGvf7Q8k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "4e205218-bf2f-47de-efe9-9fd38426927c"
      },
      "source": [
        "embedding_dim = 100\n",
        "\n",
        "mod = Sequential()\n",
        "\n",
        "mod.add(layers.Embedding(input_dim=vocab_size, \n",
        "                           output_dim=embedding_dim, \n",
        "                           input_length=maxlen,trainable=True))\n",
        "\n",
        "mod.add(layers.GlobalMaxPool1D())\n",
        "#mm.add(layers.Flatten())\n",
        "mod.add(layers.Dense(10, activation='relu'))\n",
        "#m1.add(layers.Dense(10, input_dim=X_train.shape[1], activation='relu'))\n",
        "mod.add(layers.Dense(4, activation='softmax'))\n",
        "mod.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['categorical_accuracy'])\n",
        "mod.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 50, 100)           1528700   \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_1 (Glob (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                1010      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 4)                 44        \n",
            "=================================================================\n",
            "Total params: 1,529,754\n",
            "Trainable params: 1,529,754\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJ2OfUA97Q8o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "79e68554-602f-406e-b1ab-dabcfad06669"
      },
      "source": [
        "# Fitting the model with training data\n",
        "history = mod.fit(X_train, y_train,\n",
        "                    epochs=10,\n",
        "                    verbose=1,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    batch_size=64)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 15785 samples, validate on 3947 samples\n",
            "Epoch 1/10\n",
            "15785/15785 [==============================] - 3s 185us/step - loss: 1.1440 - categorical_accuracy: 0.5394 - val_loss: 0.7507 - val_categorical_accuracy: 0.7877\n",
            "Epoch 2/10\n",
            "15785/15785 [==============================] - 1s 77us/step - loss: 0.5423 - categorical_accuracy: 0.8338 - val_loss: 0.5420 - val_categorical_accuracy: 0.8150\n",
            "Epoch 3/10\n",
            "15785/15785 [==============================] - 1s 77us/step - loss: 0.3837 - categorical_accuracy: 0.8646 - val_loss: 0.5128 - val_categorical_accuracy: 0.8178\n",
            "Epoch 4/10\n",
            "15785/15785 [==============================] - 1s 76us/step - loss: 0.3125 - categorical_accuracy: 0.8768 - val_loss: 0.5127 - val_categorical_accuracy: 0.8196\n",
            "Epoch 5/10\n",
            "15785/15785 [==============================] - 1s 77us/step - loss: 0.2725 - categorical_accuracy: 0.8825 - val_loss: 0.5223 - val_categorical_accuracy: 0.8171\n",
            "Epoch 6/10\n",
            "15785/15785 [==============================] - 1s 77us/step - loss: 0.2482 - categorical_accuracy: 0.8843 - val_loss: 0.5343 - val_categorical_accuracy: 0.8158\n",
            "Epoch 7/10\n",
            "15785/15785 [==============================] - 1s 76us/step - loss: 0.2330 - categorical_accuracy: 0.8845 - val_loss: 0.5510 - val_categorical_accuracy: 0.8130\n",
            "Epoch 8/10\n",
            "15785/15785 [==============================] - 1s 77us/step - loss: 0.2215 - categorical_accuracy: 0.8869 - val_loss: 0.5672 - val_categorical_accuracy: 0.8097\n",
            "Epoch 9/10\n",
            "15785/15785 [==============================] - 1s 76us/step - loss: 0.2138 - categorical_accuracy: 0.8878 - val_loss: 0.5807 - val_categorical_accuracy: 0.8125\n",
            "Epoch 10/10\n",
            "15785/15785 [==============================] - 1s 76us/step - loss: 0.2098 - categorical_accuracy: 0.8862 - val_loss: 0.5890 - val_categorical_accuracy: 0.8092\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7fiNzvx7Q8r"
      },
      "source": [
        "**Evaluation of the model for training and validation accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltEtIkWr7Q8r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "5c917ce1-c68c-45c9-93cb-13d0c9a3736b"
      },
      "source": [
        "loss, accuracy = mod.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy:  \",accuracy)\n",
        "loss, accuracy = mod.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy: \",accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy:   0.9095343947410583\n",
            "Testing Accuracy:  0.5554794669151306\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uc_ChRDdQ20i"
      },
      "source": [
        "- Testing Accuracy shown above is the accuracy of the model over the given validation data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w80DKDaJRI1L"
      },
      "source": [
        "#### Convolutional Neural Networks (CNN)  \n",
        "Same loss functions and metrics as the previous model, and also the output layer remains the same with Softmax activation.  \n",
        "For CNN model an extra Convolution 1-D  layer is introduced.(Only 1-D because for text processing only one dimensional layer  is required)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCMAJTdL7Q8y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "320aab3d-b69b-4e2b-929f-b3637ffe6f0c"
      },
      "source": [
        "embedding_dim = 50\n",
        "\n",
        "modelCNN = Sequential()\n",
        "modelCNN.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
        "modelCNN.add(layers.Conv1D(128, 5, activation='relu'))\n",
        "modelCNN.add(layers.GlobalMaxPooling1D())\n",
        "modelCNN.add(layers.Dense(10, activation='relu'))\n",
        "modelCNN.add(layers.Dense(4, activation='softmax'))\n",
        "modelCNN.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['categorical_accuracy'])\n",
        "modelCNN.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 50, 50)            764350    \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 46, 128)           32128     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_2 (Glob (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                1290      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 4)                 44        \n",
            "=================================================================\n",
            "Total params: 797,812\n",
            "Trainable params: 797,812\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zr_-cFar7Q81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 801
        },
        "outputId": "66d9db48-8f98-4587-d754-b8963061ccfb"
      },
      "source": [
        "history = modelCNN.fit(X_train, y_train,\n",
        "                    epochs=20,\n",
        "                    verbose=1,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    batch_size=64)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 15785 samples, validate on 3947 samples\n",
            "Epoch 1/20\n",
            "15785/15785 [==============================] - 6s 371us/step - loss: 1.0914 - categorical_accuracy: 0.4825 - val_loss: 0.8014 - val_categorical_accuracy: 0.6851\n",
            "Epoch 2/20\n",
            "15785/15785 [==============================] - 1s 76us/step - loss: 0.6160 - categorical_accuracy: 0.7780 - val_loss: 0.5597 - val_categorical_accuracy: 0.8006\n",
            "Epoch 3/20\n",
            "15785/15785 [==============================] - 1s 74us/step - loss: 0.4100 - categorical_accuracy: 0.8516 - val_loss: 0.5405 - val_categorical_accuracy: 0.8112\n",
            "Epoch 4/20\n",
            "15785/15785 [==============================] - 1s 76us/step - loss: 0.3404 - categorical_accuracy: 0.8692 - val_loss: 0.5462 - val_categorical_accuracy: 0.8100\n",
            "Epoch 5/20\n",
            "15785/15785 [==============================] - 1s 75us/step - loss: 0.3051 - categorical_accuracy: 0.8732 - val_loss: 0.5517 - val_categorical_accuracy: 0.8095\n",
            "Epoch 6/20\n",
            "15785/15785 [==============================] - 1s 76us/step - loss: 0.2780 - categorical_accuracy: 0.8792 - val_loss: 0.5799 - val_categorical_accuracy: 0.8036\n",
            "Epoch 7/20\n",
            "15785/15785 [==============================] - 1s 75us/step - loss: 0.2603 - categorical_accuracy: 0.8831 - val_loss: 0.5886 - val_categorical_accuracy: 0.8062\n",
            "Epoch 8/20\n",
            "15785/15785 [==============================] - 1s 74us/step - loss: 0.2498 - categorical_accuracy: 0.8853 - val_loss: 0.6334 - val_categorical_accuracy: 0.7966\n",
            "Epoch 9/20\n",
            "15785/15785 [==============================] - 1s 75us/step - loss: 0.2419 - categorical_accuracy: 0.8854 - val_loss: 0.6370 - val_categorical_accuracy: 0.8024\n",
            "Epoch 10/20\n",
            "15785/15785 [==============================] - 1s 76us/step - loss: 0.2345 - categorical_accuracy: 0.8857 - val_loss: 0.6288 - val_categorical_accuracy: 0.8057\n",
            "Epoch 11/20\n",
            "15785/15785 [==============================] - 1s 75us/step - loss: 0.2279 - categorical_accuracy: 0.8884 - val_loss: 0.6394 - val_categorical_accuracy: 0.8016\n",
            "Epoch 12/20\n",
            "15785/15785 [==============================] - 1s 75us/step - loss: 0.2238 - categorical_accuracy: 0.8886 - val_loss: 0.6545 - val_categorical_accuracy: 0.7998\n",
            "Epoch 13/20\n",
            "15785/15785 [==============================] - 1s 75us/step - loss: 0.2195 - categorical_accuracy: 0.8882 - val_loss: 0.6521 - val_categorical_accuracy: 0.8019\n",
            "Epoch 14/20\n",
            "15785/15785 [==============================] - 1s 76us/step - loss: 0.2167 - categorical_accuracy: 0.8891 - val_loss: 0.6590 - val_categorical_accuracy: 0.8014\n",
            "Epoch 15/20\n",
            "15785/15785 [==============================] - 1s 76us/step - loss: 0.2165 - categorical_accuracy: 0.8861 - val_loss: 0.6610 - val_categorical_accuracy: 0.8026\n",
            "Epoch 16/20\n",
            "15785/15785 [==============================] - 1s 76us/step - loss: 0.2129 - categorical_accuracy: 0.8873 - val_loss: 0.6653 - val_categorical_accuracy: 0.8036\n",
            "Epoch 17/20\n",
            "15785/15785 [==============================] - 1s 75us/step - loss: 0.2103 - categorical_accuracy: 0.8871 - val_loss: 0.6731 - val_categorical_accuracy: 0.8009\n",
            "Epoch 18/20\n",
            "15785/15785 [==============================] - 1s 74us/step - loss: 0.2092 - categorical_accuracy: 0.8884 - val_loss: 0.6636 - val_categorical_accuracy: 0.8042\n",
            "Epoch 19/20\n",
            "15785/15785 [==============================] - 1s 75us/step - loss: 0.2061 - categorical_accuracy: 0.8866 - val_loss: 0.6807 - val_categorical_accuracy: 0.8016\n",
            "Epoch 20/20\n",
            "15785/15785 [==============================] - 1s 74us/step - loss: 0.2023 - categorical_accuracy: 0.8886 - val_loss: 0.6946 - val_categorical_accuracy: 0.8001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuxB-Ox57Q87"
      },
      "source": [
        "**Evaluation of the model for training and validation accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bn117KVx7Q88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "5ac50c7f-f1f3-4f71-d424-de6cbc0fcbc8"
      },
      "source": [
        "loss, accuracy = modelCNN.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy:  \",accuracy)\n",
        "loss, accuracy = modelCNN.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  \",accuracy)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy:   0.9109280705451965\n",
            "Testing Accuracy:   0.5472602844238281\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoGQIe1RSbU5"
      },
      "source": [
        "- Testing Accuracy shown above is the accuracy of the model over the given validation data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPvfAhPBSqTR"
      },
      "source": [
        "**Best Deep Learning Model**  \n",
        "Comparing Accuracy on validation data for the above two DNN and CNN models, its clear that the first deep NN model performs better than the CNN model by approximately 0.821%. Therefore, the first deep dense neural model is selected as the best model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gBbaOGf7Q9C"
      },
      "source": [
        "**Predicting sentiments for the best NN model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIAtPOhu7Q9C"
      },
      "source": [
        "# Public Data set\n",
        "test = pub_test_data[\"Content\"].values\n",
        "XTest = tokeniz.texts_to_sequences(test) # feature extraction for public test data set\n",
        "XTest = pad_sequences(XTest, padding='post', maxlen=maxlen) # padding of maxlen =50\n",
        "\n",
        "# Private Data set\n",
        "private = priv_data[\"Content\"].values\n",
        "Xprivate = tokeniz.texts_to_sequences(private)# feature extraction for private test data set\n",
        "Xprivate = pad_sequences(Xprivate, padding='post', maxlen=maxlen) # padding of maxlen =5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSefBLuH7Q9G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "5e53910c-d5cc-4ebf-fa1a-448c7d48378e"
      },
      "source": [
        "# Predicting for public data\n",
        "ypred_public = mod.predict_classes(XTest,verbose=0)\n",
        "writeToCSV(ypred_public)\n",
        "\n",
        "# Predicting for private data\n",
        "ypred_private = mod.predict_classes(Xprivate,verbose=0)\n",
        "writeToCSVpriv(ypred_private)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    Prediction\n",
            "ID            \n",
            "1            3\n",
            "2            2\n",
            "3            0\n",
            "4            3\n",
            "5            1\n",
            "    Prediction\n",
            "ID            \n",
            "0            2\n",
            "1            2\n",
            "2            1\n",
            "3            1\n",
            "4            1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8GZpnCv7Q9K"
      },
      "source": [
        "## Discussion of Model Performance and Implementation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cpn7VAHX7Q9L"
      },
      "source": [
        "Comparing the results of the Conventional model and Deep Learning model, the deep learning one exhibited better accuracy of _ more than the other model on the public data set. the deep learning model ranked 2nd in Kaggle submissions on public data set with an accuracy of 72.2%.\n",
        "\n",
        "Both conventional and deep learning models performed very well on public data set than the validation data set with a difference of ~(12-18)% and among them deep learning model exhibited better predictions/classifications.\n",
        "Similarly, both conventional and deep learning models performed very well with private data set compared to validation set, with a gain in accuracy of ~(8-10)% but less than the accuracy acheived in public data set.\n",
        "Thus, both models performed much better with public data set.\n",
        "\n",
        "In conclusion, both conventional and deep learning models can deliver better  performance with large training data sets, careful preprocessing and efficient optimization of the models(like selecting hyperparameters)."
      ]
    }
  ]
}